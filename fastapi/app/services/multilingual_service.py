"""
Vietnamese Summarization Service - Using Finetuned ViT5
Model Ä‘Æ°á»£c fine-tune vá»›i prefix "lÃ m mÆ°á»£t: " Ä‘á»ƒ viáº¿t láº¡i vÄƒn báº£n mÆ°á»£t mÃ .
"""

import logging
import random
import re
from typing import Optional, Tuple, List

import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

from app.utils.text_processor import TextProcessor, get_text_processor

logger = logging.getLogger(__name__)


class MultilingualSummarizationService:
    """
    Vietnamese summarization service using local finetuned ViT5.
    
    Model Ä‘Æ°á»£c fine-tune vá»›i prefix "lÃ m mÆ°á»£t: " Ä‘á»ƒ viáº¿t láº¡i vÄƒn báº£n.
    
    QUAN TRá»ŒNG: Prefix "lÃ m mÆ°á»£t: " pháº£i Ä‘Æ°á»£c thÃªm vÃ o Ä‘áº§u input,
    náº¿u khÃ´ng model sáº½ khÃ´ng hoáº¡t Ä‘á»™ng Ä‘Ãºng.
    """
    
    MODEL_NAME = "AI_Models/my_vit5_model"
    PREFIX = "lÃ m mÆ°á»£t: "  # Prefix báº¯t buá»™c - model Ä‘Æ°á»£c train vá»›i prefix nÃ y
    
    def __init__(self):
        self._model = None
        self._tokenizer = None
        self._device = "cuda" if torch.cuda.is_available() else "cpu"
        self._text_processor: TextProcessor = get_text_processor()
        
        logger.info(f"ViT5 SummarizationService initialized. Device: {self._device}")
    
    def _load_model(self) -> None:
        """Lazy load ViT5 model"""
        if self._model is None:
            logger.info(f"Loading {self.MODEL_NAME}... (this may take 1-2 minutes)")
            self._tokenizer = AutoTokenizer.from_pretrained(self.MODEL_NAME)
            self._model = AutoModelForSeq2SeqLM.from_pretrained(self.MODEL_NAME)
            self._model.to(self._device)
            self._model.eval()
            logger.info(f"{self.MODEL_NAME} loaded successfully!")
    
    def summarize(
        self,
        text: str,
        max_length: int = 150,
        min_length: int = 30,
        language: str = "vi"
    ) -> Tuple[str, str]:
        """
        Summarize Vietnamese text.
        
        Args:
            text: Input Vietnamese text to summarize
            max_length: Maximum summary length (in tokens)
            min_length: Minimum summary length (in tokens)
            language: Language hint (vi for Vietnamese)
            
        Returns:
            Tuple[str, str]: (raw_summary, processed_summary)
        """
        self._load_model()
        
        # PRE-PROCESSING
        cleaned_text = self._text_processor.preprocess(text)
        
        # QUAN TRá»ŒNG: ThÃªm prefix "lÃ m mÆ°á»£t: " - model Ä‘Æ°á»£c train vá»›i prefix nÃ y
        # Thiáº¿u prefix thÃ¬ model sáº½ tráº£ káº¿t quáº£ linh tinh
        input_text = self.PREFIX + cleaned_text
        
        inputs = self._tokenizer(
            input_text,
            return_tensors="pt",
            max_length=1024,
            truncation=True
        ).to(self._device)
        
        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_length=max_length,
                min_length=min_length,
                num_beams=4,
                length_penalty=2.0,
                early_stopping=True,
                no_repeat_ngram_size=3,
            )
        
        raw_summary = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # POST-PROCESSING (lighter for Vietnamese)
        processed_summary = raw_summary.strip()
        
        # Ensure proper ending
        if processed_summary and processed_summary[-1] not in '.!?':
            processed_summary += '.'
        
        return raw_summary, processed_summary
    
    def smooth_sentence(self, sentence: str) -> str:
        """
        LÃ m mÆ°á»£t Má»˜T cÃ¢u Ä‘Æ¡n láº».
        
        ÄÃ¢y lÃ  cÃ¡ch Ä‘Ãºng Ä‘á»ƒ dÃ¹ng model vÃ¬ model Ä‘Æ°á»£c train vá»›i input lÃ  1 cÃ¢u.
        CÃ³ cÆ¡ cháº¿ "Báº£o hiá»ƒm Sá»‘ liá»‡u" - náº¿u ViT5 lÃ m máº¥t sá»‘ thÃ¬ fallback vá» cÃ¢u gá»‘c.
        
        Args:
            sentence: Má»™t cÃ¢u tiáº¿ng Viá»‡t cáº§n lÃ m mÆ°á»£t
            
        Returns:
            str: CÃ¢u Ä‘Ã£ Ä‘Æ°á»£c lÃ m mÆ°á»£t (hoáº·c cÃ¢u gá»‘c náº¿u ViT5 lÃ m máº¥t thÃ´ng tin)
        """
        self._load_model()
        
        original_sentence = sentence.strip()
        
        # Bá» qua cÃ¢u quÃ¡ ngáº¯n
        if len(original_sentence) < 10:
            return original_sentence
        
        # ThÃªm prefix báº¯t buá»™c
        input_text = self.PREFIX + original_sentence
        
        inputs = self._tokenizer(
            input_text,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self._device)
        
        # TÃ­nh min_length Ä‘á»™ng: Ã­t nháº¥t 70% Ä‘á»™ dÃ i input (token)
        # Äiá»u nÃ y NGÄ‚N model cáº¯t xÃ©n quÃ¡ Ä‘Ã 
        input_token_count = inputs["input_ids"].shape[1]
        dynamic_min_length = max(20, int(input_token_count * 0.7))
        
        with torch.no_grad():
            outputs = self._model.generate(
                inputs["input_ids"],
                max_length=512,  # Cho phÃ©p viáº¿t dÃ i
                min_length=dynamic_min_length,  # Ã‰p viáº¿t Ä‘á»§ dÃ i
                num_beams=8,  # TÃ¬m kiáº¿m ká»¹ hÆ¡n
                length_penalty=2.0,  # Khuyáº¿n khÃ­ch viáº¿t dÃ i
                repetition_penalty=1.2,  # TrÃ¡nh láº·p tá»«
                no_repeat_ngram_size=2,  # Giáº£m xuá»‘ng Ä‘á»ƒ khÃ´ng cáº¯t nháº§m
                early_stopping=True
            )
        
        result = self._tokenizer.decode(outputs[0], skip_special_tokens=True)
        result = result.strip()
        
        # XÃ³a cÃ¡c kÃ½ tá»± thá»«a á»Ÿ Ä‘áº§u (*, -, sá»‘ thá»© tá»±...)
        result = result.lstrip('*-â€¢â€“â€”. ')
        # NhÆ°ng KHÃ”NG lstrip sá»‘ vÃ¬ sá»‘ cÃ³ thá»ƒ lÃ  thÃ´ng tin quan trá»ng!
        
        # CÆ  CHáº¾ "Báº¢O HIá»‚M Sá» LIá»†U"
        # Náº¿u cÃ¢u gá»‘c cÃ³ sá»‘ mÃ  ViT5 lÃ m máº¥t -> Fallback vá» cÃ¢u gá»‘c
        result = self._safety_check_numbers(original_sentence, result)
        
        # Äáº£m báº£o káº¿t thÃºc Ä‘Ãºng
        if result and result[-1] not in '.!?':
            result += '.'
            
        return result
    
    def _safety_check_numbers(self, original: str, generated: str) -> str:
        """
        CÆ¡ cháº¿ "Báº£o hiá»ƒm Sá»‘ liá»‡u" nÃ¢ng cao.
        
        Kiá»ƒm tra 2 loáº¡i lá»—i:
        1. Máº¥t sá»‘: CÃ¢u gá»‘c cÃ³ "12.000" mÃ  ViT5 bá» máº¥t
        2. Sai sá»‘ thá»© tá»±: "Giai Ä‘oáº¡n 1" -> "Giai Ä‘oáº¡n 2" (NGUY HIá»‚M!)
        
        -> Náº¿u phÃ¡t hiá»‡n lá»—i: Vá»©t cÃ¢u ViT5, dÃ¹ng láº¡i cÃ¢u gá»‘c.
        
        Args:
            original: CÃ¢u gá»‘c tá»« PhoBERT
            generated: CÃ¢u ViT5 vá»«a sinh ra
            
        Returns:
            str: CÃ¢u an toÃ n (khÃ´ng bá»‹ sai lá»‡ch sá»‘ liá»‡u)
        """
        # ========== CHECK 1: Máº¥t sá»‘ ==========
        numbers_original = set(re.findall(r'\d+', original))
        numbers_generated = set(re.findall(r'\d+', generated))
        
        if numbers_original and not numbers_original.issubset(numbers_generated):
            missing_numbers = numbers_original - numbers_generated
            logger.warning(
                f"âš ï¸ ViT5 lÃ m máº¥t sá»‘ liá»‡u: {missing_numbers}. Fallback vá» cÃ¢u gá»‘c."
            )
            return original
        
        # ========== CHECK 2: Sai sá»‘ thá»© tá»± (Ordinal Hallucination) ==========
        # TÃ¬m pattern "Giai Ä‘oáº¡n X", "BÆ°á»›c X", "Pháº§n X", "Cáº¥p X", "Lá»›p X"
        ordinal_patterns = [
            r'giai\s*Ä‘oáº¡n\s*(\d+)',
            r'bÆ°á»›c\s*(\d+)',
            r'pháº§n\s*(\d+)',
            r'cáº¥p\s*(\d+)',
            r'lá»›p\s*(\d+)',
            r'nÄƒm\s*(\d{4})',  # NÄƒm cÅ©ng quan trá»ng
        ]
        
        for pattern in ordinal_patterns:
            orig_matches = re.findall(pattern, original.lower())
            gen_matches = re.findall(pattern, generated.lower())
            
            # Náº¿u cÃ¢u gá»‘c cÃ³ ordinal mÃ  cÃ¢u sinh ra cÃ³ ordinal KHÃC -> Lá»—i!
            if orig_matches and gen_matches:
                # So sÃ¡nh xem cÃ³ bá»‹ Ä‘á»•i sá»‘ khÃ´ng
                for orig_num in orig_matches:
                    if orig_num not in gen_matches:
                        # Sá»‘ thá»© tá»± bá»‹ thay Ä‘á»•i -> Nguy hiá»ƒm!
                        logger.warning(
                            f"ğŸš¨ ViT5 sai sá»‘ thá»© tá»±: '{pattern}' {orig_num} -> {gen_matches}. "
                            f"Fallback vá» cÃ¢u gá»‘c."
                        )
                        return original
        
        # ========== CHECK 3: Sá»‘ má»›i xuáº¥t hiá»‡n mÃ  khÃ´ng cÃ³ trong gá»‘c ==========
        # Náº¿u ViT5 tá»± bá»‹a ra sá»‘ má»›i -> CÅ©ng lÃ  hallucination
        new_numbers = numbers_generated - numbers_original
        # Chá»‰ cáº£nh bÃ¡o náº¿u sá»‘ má»›i lÃ  sá»‘ quan trá»ng (> 2 chá»¯ sá»‘ hoáº·c lÃ  nÄƒm)
        suspicious_new = [n for n in new_numbers if len(n) >= 2 or int(n) > 100]
        if suspicious_new and numbers_original:
            logger.warning(
                f"âš ï¸ ViT5 tá»± bá»‹a sá»‘ má»›i: {suspicious_new}. Fallback vá» cÃ¢u gá»‘c."
            )
            return original
        
        return generated  # Äá»§ an toÃ n, dÃ¹ng cÃ¢u ViT5
    
    def smooth_sentences(self, sentences: list) -> str:
        """
        LÃ m mÆ°á»£t Tá»ªNG CÃ‚U trong list, rá»“i ghÃ©p láº¡i thÃ nh Ä‘oáº¡n vÄƒn liá»n máº¡ch.
        
        Pipeline "Chia Ä‘á»ƒ trá»‹" nÃ¢ng cao:
        1. Xá»­ lÃ½ tá»«ng cÃ¢u riÃªng láº» qua ViT5
        2. Thay tháº¿ tá»« láº·p báº±ng tá»« Ä‘á»“ng nghÄ©a (Dynamic Synonym)
        3. ThÃªm tá»« ná»‘i dá»±a trÃªn sentiment (Sentiment-based Linking)
        4. Háº­u xá»­ lÃ½ Ä‘á»ƒ fix format + polish vÄƒn báº£n
        
        Args:
            sentences: List cÃ¡c cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c PhoBERT trÃ­ch xuáº¥t
            
        Returns:
            str: Äoáº¡n vÄƒn mÆ°á»£t mÃ , liá»n máº¡ch
        """
        self._load_model()
        
        smoothed_parts = []
        
        for sent in sentences:
            sent = sent.strip()
            if len(sent) > 10:  # Bá» qua cÃ¢u rÃ¡c quÃ¡ ngáº¯n
                smooth = self.smooth_sentence(sent)
                if smooth:
                    smoothed_parts.append(smooth)
        
        # ========== BÆ¯á»šC 2: DYNAMIC SYNONYM REPLACEMENT ==========
        # Thay tháº¿ tá»« láº·p báº±ng tá»« Ä‘á»“ng nghÄ©a Ä‘á»ƒ trÃ¡nh monotonous
        smoothed_parts = self._dynamic_synonym_replace(smoothed_parts)
        
        # ========== BÆ¯á»šC 3: SENTIMENT-BASED LINKING ==========
        # ThÃªm tá»« ná»‘i phÃ¹ há»£p giá»¯a cÃ¡c cÃ¢u dá»±a trÃªn sentiment
        smoothed_parts = self._add_dynamic_connectors(smoothed_parts)
        
        # GhÃ©p láº¡i thÃ nh Ä‘oáº¡n vÄƒn
        final_text = " ".join(smoothed_parts)
        
        # Háº­u xá»­ lÃ½: XÃ³a cÃ¡c tá»« ná»‘i vÃ´ nghÄ©a á»Ÿ Ä‘áº§u cÃ¢u Ä‘áº§u tiÃªn
        unwanted_starts = [
            "Theo Ä‘Ã³,", "Theo Ä‘Ã³ ", 
            "BÃªn cáº¡nh Ä‘Ã³,", "BÃªn cáº¡nh Ä‘Ã³ ",
            "ThÃªm vÃ o Ä‘Ã³,", "ThÃªm vÃ o Ä‘Ã³ ",
            "NgoÃ i ra,", "NgoÃ i ra ",
            "Do Ä‘Ã³,", "Do Ä‘Ã³ ",
        ]
        for phrase in unwanted_starts:
            if final_text.startswith(phrase):
                final_text = final_text[len(phrase):].strip()
                break
        
        # Viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u
        if final_text:
            final_text = final_text[0].upper() + final_text[1:]
        
        # ÃP Dá»¤NG POST-PROCESSING POLISH (Fix format -> 10/10)
        final_text = self._post_process_polish(final_text)
        
        return final_text
    
    # ==================== KHO Tá»ª ÄIá»‚N Äá»’NG NGHÄ¨A Äá»˜NG ====================
    
    # Kho tá»« Ä‘iá»ƒn Ä‘á»“ng nghÄ©a (dá»… má»Ÿ rá»™ng)
    SYNONYM_DICT = {
        "cho biáº¿t": ["nháº­n Ä‘á»‹nh", "chia sáº»", "Ä‘Ã¡nh giÃ¡", "kháº³ng Ä‘á»‹nh", "nháº¥n máº¡nh", "nÃªu rÃµ"],
        "nÃ³i ráº±ng": ["cho hay", "phÃ¡t biá»ƒu", "bÃ y tá»", "tuyÃªn bá»‘"],
        "dá»± kiáº¿n": ["theo káº¿ hoáº¡ch", "Ä‘Æ°á»£c ká»³ vá»ng", "Æ°á»›c tÃ­nh"],
        "triá»ƒn khai": ["thá»±c hiá»‡n", "Ã¡p dá»¥ng", "tiáº¿n hÃ nh"],
        "Ä‘Ã¡nh giÃ¡": ["nháº­n xÃ©t", "Ä‘Ã¡nh giÃ¡", "ghi nháº­n"],
    }
    
    def _dynamic_synonym_replace(self, sentences: List[str]) -> List[str]:
        """
        Thay tháº¿ tá»« láº·p báº±ng tá»« Ä‘á»“ng nghÄ©a (Dynamic Synonym Replacement).
        
        Logic:
        - Theo dÃµi cÃ¡c tá»« Ä‘Ã£ dÃ¹ng trong cÃ¡c cÃ¢u trÆ°á»›c
        - Náº¿u cÃ¢u hiá»‡n táº¡i cÃ³ tá»« Ä‘Ã£ dÃ¹ng -> Thay tháº¿ báº±ng tá»« Ä‘á»“ng nghÄ©a random
        - Reset bá»™ nhá»› sau má»—i 3 cÃ¢u Ä‘á»ƒ tá»± nhiÃªn hÆ¡n
        
        Args:
            sentences: List cÃ¡c cÃ¢u Ä‘Ã£ lÃ m mÆ°á»£t
            
        Returns:
            List[str]: CÃ¡c cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c thay tháº¿ tá»« láº·p
        """
        refined_sentences = []
        used_words = set()
        
        for i, sent in enumerate(sentences):
            # Reset bá»™ nhá»› sau má»—i 3 cÃ¢u Ä‘á»ƒ vÄƒn phong tá»± nhiÃªn
            if i % 3 == 0:
                used_words.clear()
            
            new_sent = sent
            
            for key, replacements in self.SYNONYM_DICT.items():
                # Kiá»ƒm tra tá»« khÃ³a trong cÃ¢u (khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng)
                if key.lower() in new_sent.lower():
                    # Náº¿u tá»« nÃ y vá»«a dÃ¹ng á»Ÿ cÃ¢u trÆ°á»›c -> THAY THáº¾
                    if key in used_words:
                        # Chá»n random 1 tá»« thay tháº¿ chÆ°a dÃ¹ng
                        available = [w for w in replacements if w not in used_words]
                        if available:
                            replacement = random.choice(available)
                            # Thay tháº¿ (giá»¯ nguyÃªn viáº¿t hoa náº¿u á»Ÿ Ä‘áº§u cÃ¢u)
                            pattern = re.compile(re.escape(key), re.IGNORECASE)
                            new_sent = pattern.sub(replacement, new_sent, count=1)
                            used_words.add(replacement)
                            logger.debug(f"ğŸ”„ Synonym: '{key}' -> '{replacement}'")
                    else:
                        used_words.add(key)
            
            refined_sentences.append(new_sent)
        
        return refined_sentences
    
    # ==================== SENTIMENT-BASED LINKING ====================
    
    # Tá»« khÃ³a Ä‘á»ƒ phÃ¡t hiá»‡n sentiment
    POSITIVE_KEYWORDS = [
        "tá»‘t", "quan trá»ng", "á»§ng há»™", "Ä‘Ã¡nh giÃ¡ cao", "lá»£i Ã­ch", 
        "thÃ nh cÃ´ng", "hiá»‡u quáº£", "tiáº¿n bá»™", "phÃ¡t triá»ƒn", "thuáº­n lá»£i",
        "Æ°u Ä‘iá»ƒm", "cáº£i thiá»‡n", "nÃ¢ng cao", "khuyáº¿n khÃ­ch"
    ]
    
    NEGATIVE_KEYWORDS = [
        "lo ngáº¡i", "khÃ³ khÄƒn", "thiáº¿u", "háº¡n cháº¿", "rÃ o cáº£n",
        "tuy nhiÃªn", "nhÆ°á»£c Ä‘iá»ƒm", "thÃ¡ch thá»©c", "váº¥n Ä‘á»", "khÃ´ng Ä‘á»§",
        "chÆ°a", "máº·c dÃ¹", "trá»Ÿ ngáº¡i", "báº¥t cáº­p"
    ]
    
    # Tá»« ná»‘i theo sentiment transition
    CONNECTORS = {
        "positive_to_negative": ["Tuy nhiÃªn,", "Máº·c dÃ¹ váº­y,", "Song,", "Dáº«u váº­y,"],
        "negative_to_positive": ["Máº·t khÃ¡c,", "TrÃ¡i láº¡i,", "NgÆ°á»£c láº¡i,"],
        "same_positive": ["Äá»“ng thá»i,", "BÃªn cáº¡nh Ä‘Ã³,", "NgoÃ i ra,", "HÆ¡n ná»¯a,"],
        "same_negative": ["ThÃªm vÃ o Ä‘Ã³,", "ÄÃ¡ng lo ngáº¡i hÆ¡n,", "CÅ©ng cÃ³ Ã½ kiáº¿n ráº±ng,"],
        "neutral": ["Theo Ä‘Ã³,", "Cá»¥ thá»ƒ,", "Vá» máº·t nÃ y,"]
    }
    
    def _get_sentiment(self, text: str) -> int:
        """
        PhÃ¢n tÃ­ch sentiment Ä‘Æ¡n giáº£n dá»±a trÃªn tá»« khÃ³a.
        
        Returns:
            1: TÃ­ch cá»±c (Positive)
            -1: TiÃªu cá»±c (Negative)
            0: Trung tÃ­nh (Neutral)
        """
        text_lower = text.lower()
        
        pos_count = sum(1 for kw in self.POSITIVE_KEYWORDS if kw in text_lower)
        neg_count = sum(1 for kw in self.NEGATIVE_KEYWORDS if kw in text_lower)
        
        if pos_count > neg_count:
            return 1
        elif neg_count > pos_count:
            return -1
        return 0
    
    def _add_dynamic_connectors(self, sentences: List[str]) -> List[str]:
        """
        ThÃªm tá»« ná»‘i phÃ¹ há»£p giá»¯a cÃ¡c cÃ¢u dá»±a trÃªn sentiment.
        
        Logic nÃ¢ng cao:
        - PhÃ¢n tÃ­ch sentiment cá»§a cÃ¢u trÆ°á»›c vÃ  cÃ¢u hiá»‡n táº¡i
        - Náº¿u chuyá»ƒn tá»« positive -> negative: ThÃªm "Tuy nhiÃªn,"
        - Náº¿u cÃ¢u báº¯t Ä‘áº§u báº±ng "BÃªn cáº¡nh" nhÆ°ng sentiment Ä‘á»•i chiá»u: Thay báº±ng "Tuy nhiÃªn,"
        - Náº¿u cÃ¹ng chiá»u: ThÃªm "Äá»“ng thá»i," hoáº·c "NgoÃ i ra,"
        
        Args:
            sentences: List cÃ¡c cÃ¢u
            
        Returns:
            List[str]: CÃ¡c cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c thÃªm/sá»­a tá»« ná»‘i
        """
        if len(sentences) <= 1:
            return sentences
        
        # Tá»« ná»‘i Yáº¾U cáº§n Ä‘Æ°á»£c thay tháº¿ khi sentiment thay Ä‘á»•i
        WEAK_CONNECTORS = [
            "bÃªn cáº¡nh Ä‘Ã³", "bÃªn cáº¡nh", "ngoÃ i ra", "thÃªm vÃ o Ä‘Ã³", 
            "Ä‘á»“ng thá»i", "hÆ¡n ná»¯a"
        ]
        
        result = [sentences[0]]  # CÃ¢u Ä‘áº§u tiÃªn giá»¯ nguyÃªn
        
        for i in range(1, len(sentences)):
            prev_sent = sentences[i - 1]
            curr_sent = sentences[i]
            
            # Láº¥y sentiment
            prev_sentiment = self._get_sentiment(prev_sent)
            curr_sentiment = self._get_sentiment(curr_sent)
            
            # ========== Xá»¬ LÃ Äáº¶C BIá»†T: "BÃªn cáº¡nh" khi sentiment Ä‘á»•i chiá»u ==========
            # Náº¿u cÃ¢u báº¯t Ä‘áº§u báº±ng tá»« ná»‘i yáº¿u nhÆ°ng sentiment Ä‘á»•i chiá»u -> Thay tháº¿
            curr_lower = curr_sent.lower()
            starts_with_weak = any(curr_lower.startswith(wc) for wc in WEAK_CONNECTORS)
            
            if starts_with_weak and prev_sentiment > 0 and curr_sentiment < 0:
                # TÃ¬m vÃ  thay tháº¿ tá»« ná»‘i yáº¿u báº±ng tá»« ná»‘i máº¡nh
                for wc in WEAK_CONNECTORS:
                    if curr_lower.startswith(wc):
                        # XÃ³a tá»« ná»‘i yáº¿u
                        rest_of_sentence = curr_sent[len(wc):].lstrip(", ")
                        # ThÃªm tá»« ná»‘i máº¡nh
                        strong_connector = random.choice(self.CONNECTORS["positive_to_negative"])
                        curr_sent = f"{strong_connector} {rest_of_sentence[0].lower() + rest_of_sentence[1:] if rest_of_sentence else ''}"
                        logger.debug(f"ğŸ”„ Replaced weak connector '{wc}' with '{strong_connector}'")
                        break
                result.append(curr_sent)
                continue
            
            # ========== LOGIC CONNECTOR THÃ”NG THÆ¯á»œNG ==========
            # Kiá»ƒm tra xem cÃ¢u hiá»‡n táº¡i Ä‘Ã£ cÃ³ tá»« ná»‘i chÆ°a
            has_connector = any(
                curr_sent.lower().startswith(conn.lower().rstrip(','))
                for connectors in self.CONNECTORS.values()
                for conn in connectors
            )
            
            # Chá»‰ thÃªm tá»« ná»‘i náº¿u cÃ¢u chÆ°a cÃ³
            if not has_connector:
                connector = ""
                
                # Quyáº¿t Ä‘á»‹nh loáº¡i tá»« ná»‘i dá»±a trÃªn sentiment transition
                if prev_sentiment > 0 and curr_sentiment < 0:
                    connector = random.choice(self.CONNECTORS["positive_to_negative"])
                elif prev_sentiment < 0 and curr_sentiment > 0:
                    connector = random.choice(self.CONNECTORS["negative_to_positive"])
                elif prev_sentiment == curr_sentiment and prev_sentiment > 0:
                    # Chá»‰ thÃªm tá»« ná»‘i cho má»™t sá»‘ cÃ¢u (khÃ´ng pháº£i táº¥t cáº£) Ä‘á»ƒ tá»± nhiÃªn
                    if random.random() < 0.6:  # 60% chance
                        connector = random.choice(self.CONNECTORS["same_positive"])
                elif prev_sentiment == curr_sentiment and prev_sentiment < 0:
                    if random.random() < 0.5:  # 50% chance
                        connector = random.choice(self.CONNECTORS["same_negative"])
                
                if connector:
                    # ThÃªm tá»« ná»‘i vÃ  viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u tiÃªn cá»§a cÃ¢u gá»‘c
                    curr_sent_adjusted = curr_sent[0].lower() + curr_sent[1:] if curr_sent else curr_sent
                    curr_sent = f"{connector} {curr_sent_adjusted}"
                    logger.debug(f"ğŸ”— Added connector: '{connector}'")
            
            result.append(curr_sent)
        
        return result
    
    def _post_process_polish(self, text: str) -> str:
        """
        Háº­u xá»­ lÃ½ Ä‘á»ƒ Ä‘áº¡t Ä‘iá»ƒm 10/10.
        
        Fix 2 lá»—i format phá»• biáº¿n:
        1. Viáº¿t hoa sau dáº¥u cháº¥m (". b" -> ". B")
        2. Sá»‘ liá»‡u bá»‹ tÃ¡ch (4. 0 -> 4.0, 25. 000 -> 25.000)
        
        Args:
            text: VÄƒn báº£n cáº§n polish
            
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c polish hoÃ n háº£o
        """
        if not text:
            return text
        
        # 1. Fix lá»—i sá»‘ liá»‡u bá»‹ tÃ¡ch (PHáº¢I LÃ€M TRÆ¯á»šC)
        # 4. 0 -> 4.0, 25. 000 -> 25.000
        text = re.sub(r'(\d+)\.\s+(\d+)', r'\1.\2', text)
        
        # 2. Fix lá»—i viáº¿t hoa sau dáº¥u cháº¥m
        # ". b" -> ". B"
        def capitalize_after_period(match):
            return match.group(1) + match.group(2).upper()
        
        text = re.sub(r'(\.\s+)([a-zÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ä‘Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µ])', 
                      capitalize_after_period, text)
        
        return text
    
    def get_model_info(self) -> dict:
        """Return information about the model"""
        return {
            "model_name": self.MODEL_NAME,
            "prefix": self.PREFIX,
            "description": "ViT5 fine-tuned vá»›i prefix 'lÃ m mÆ°á»£t:' Ä‘á»ƒ viáº¿t láº¡i vÄƒn báº£n mÆ°á»£t mÃ ",
            "supported_languages": ["vi"],
            "model_size": "~900MB",
            "loaded": self._model is not None,
            "type": "local finetuned",
            "note": "NÃªn dÃ¹ng smooth_sentences() Ä‘á»ƒ xá»­ lÃ½ tá»«ng cÃ¢u riÃªng láº»"
        }


# Singleton instance for dependency injection
_multilingual_service: Optional[MultilingualSummarizationService] = None


def get_multilingual_service() -> MultilingualSummarizationService:
    """Get or create MultilingualSummarizationService singleton"""
    global _multilingual_service
    if _multilingual_service is None:
        _multilingual_service = MultilingualSummarizationService()
    return _multilingual_service
